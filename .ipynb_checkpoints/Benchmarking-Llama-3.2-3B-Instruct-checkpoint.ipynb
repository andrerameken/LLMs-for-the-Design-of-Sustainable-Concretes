{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc561d7",
   "metadata": {},
   "source": [
    "### Llama-3.2-3B-Instruct: Few-shot prompting given domain context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded80675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils import *\n",
    "from openai import RateLimitError\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "# model name and/or local path to model\n",
    "model_name = \"Llama-3.2-3B-Instruct\"\n",
    "# Set to None if not locally downloaded\n",
    "model_path = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path if model_path is not None else model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype = \"auto\",\n",
    "                                             device_map = \"auto\")\n",
    "\n",
    "# Create messages\n",
    "\n",
    "data_path = os.path.join('data', 'transformed_data.txt')\n",
    "with open(data_path, 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "system_message_path = os.path.join(\"data\", \"system_message_context.txt\")\n",
    "with open(system_message_path, \"r\") as f:\n",
    "    system_txt = f.read().strip()\n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": system_txt}\n",
    "\n",
    "# Randomly sample n lines for training and N lines for testing\n",
    "n = 10\n",
    "N = 25\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "result_list = []\n",
    "indices = list(range(len(data)))\n",
    "# Repeat the process 10 times\n",
    "for i in range(10):\n",
    "    random.seed(i)\n",
    "    #np.random.seed(i)\n",
    "    test_prompts, true_values, predictions = gather_LLM_results(data,\n",
    "                                                                n,\n",
    "                                                                N,\n",
    "                                                                None,\n",
    "                                                                model_name,\n",
    "                                                                indices,\n",
    "                                                                system_message,\n",
    "                                                                None,\n",
    "                                                                model,\n",
    "                                                                tokenizer)\n",
    "\n",
    "    append_to_result_list(test_prompts, true_values, predictions, result_list)\n",
    "\n",
    "save_results_to_csv(model_name, \"ICL_finetuned\", result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a475b6",
   "metadata": {},
   "source": [
    "### Llama-3.2-3B-Instruct: few-shot prompting without domain context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acff497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils import *\n",
    "from openai import RateLimitError\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "# model name and/or local path to model\n",
    "model_name = \"Llama-3.2-3B-Instruct\"\n",
    "# Set to None if not locally downloaded\n",
    "model_path = None\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path if model_path is not None else model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype = \"auto\",\n",
    "                                             device_map = \"auto\")\n",
    "\n",
    "# Create messages\n",
    "\n",
    "data_path = os.path.join('data', 'transformed_data.txt')\n",
    "with open(data_path, 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "system_message_path = os.path.join(\"data\", \"system_message.txt\")\n",
    "with open(system_message_path, \"r\") as f:\n",
    "    system_txt = f.read().strip()\n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": system_txt}\n",
    "\n",
    "# Randomly sample n lines for training and N lines for testing\n",
    "n = 10\n",
    "N = 25\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "result_list = []\n",
    "indices = list(range(len(data)))\n",
    "# Repeat the process 10 times\n",
    "for i in range(10):\n",
    "    random.seed(i)\n",
    "    #np.random.seed(i)\n",
    "    test_prompts, true_values, predictions = gather_LLM_results(data,\n",
    "                                                                n,\n",
    "                                                                N,\n",
    "                                                                None,\n",
    "                                                                model_name,\n",
    "                                                                indices,\n",
    "                                                                system_message,\n",
    "                                                                None,\n",
    "                                                                model,\n",
    "                                                                tokenizer)\n",
    "\n",
    "    append_to_result_list(test_prompts, true_values, predictions, result_list)\n",
    "\n",
    "save_results_to_csv(model_name, \"ICL\", result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b2530",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression (GPR)\n",
    "\n",
    "In this section, we will benchmark the performance of the Gaussian Process Regression (GPR) model using the `scikit-learn` library. GPR is a non-parametric, Bayesian approach to regression that provides uncertainty estimates of the predictions. It is based on the assumption that any finite set of data points can be modeled by a multivariate Gaussian distribution.\n",
    "\n",
    "All training and test sets used for the experiments will be stored in the `results` folder, allowing for easy access and reproducibility of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b166d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "\n",
    "\n",
    "#data = pd.read_csv(r'data/numeric_data.csv')\n",
    "file_path = os.path.join('data', 'numeric_data.csv')\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(length_scale=10, nu=1.5)\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "# DataFrames to store results\n",
    "train_results_df = pd.DataFrame()\n",
    "test_results_df = pd.DataFrame(columns=['Iteration', 'Idx_Sample', 'Input Features', 'True Values', 'Predicted Values'])\n",
    "\n",
    "n = 10\n",
    "N = 25\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "for i in range(10):\n",
    "    random.seed(i)\n",
    "    #np.random.seed(i)\n",
    "    #random_indices = np.random.choice(len(data), n+N, False)\n",
    "    random.shuffle(indices)\n",
    "    # Sample the data based on the provided indices\n",
    "    train_data = data.iloc[indices[:n]]\n",
    "    test_data = data.iloc[indices[n:n+N]]\n",
    "\n",
    "    target_column = 'fc_28dGroundTruth'\n",
    "    idx_column = 'Idx_Sample'\n",
    "    X_train = train_data.drop(columns=[target_column, idx_column], axis=1)\n",
    "\n",
    "    # Normalize input features\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train = X_scaler.fit_transform(X_train)\n",
    "\n",
    "    # Scale the target variable for training\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train = y_scaler.fit_transform(train_data[target_column].copy().to_numpy().reshape(-1, 1))\n",
    "\n",
    "    gpr.fit(X_train, y_train)\n",
    "\n",
    "    # Test data\n",
    "    X_test = test_data.drop(columns=[target_column, idx_column], axis=1)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "\n",
    "    # Predict on test data\n",
    "    predictions = gpr.predict(X_test)\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    predictions = y_scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Store true and predicted values\n",
    "    true_values = test_data[target_column].copy().to_numpy().reshape(-1, 1)\n",
    "    idx_sample = test_data[idx_column].copy().to_numpy()\n",
    "\n",
    "    # Store train data\n",
    "    \n",
    "    train_results_df = pd.concat([train_results_df, train_data], ignore_index=True)\n",
    "\n",
    "    # Store test data\n",
    "    iteration_df = pd.DataFrame({\n",
    "        'Iteration': i+1,\n",
    "        'Idx_Sample': idx_sample,\n",
    "        'Input Features': list(X_test),\n",
    "        'True Values': true_values.flatten(),\n",
    "        'Predicted Values': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    test_results_df = pd.concat([test_results_df, iteration_df], ignore_index=True)\n",
    "\n",
    "    # Calculate R2 score and mean absolute error\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    mae = mean_absolute_error(true_values, predictions)   \n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"Iteration: {i+1}\")\n",
    "    print(f\"R-squared: {r2:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "\n",
    "\n",
    "train_results_file = os.path.join('results', model_name, 'GPR', 'train.csv')\n",
    "\n",
    "# Make needed directories\n",
    "dir_name = os.path.dirname(train_results_file)\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "train_results_df.to_csv(train_results_file, index=False)\n",
    "\n",
    "test_results_file = os.path.join('results', model_name, 'GPR', 'test.csv')\n",
    "test_results_df.to_csv(test_results_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08adf38",
   "metadata": {},
   "source": [
    "### Random Forest (M5-Tree with Linear Tree Models)\n",
    "\n",
    "In this section, we will benchmark the performance of the Random Forest (RF) model using an M5-Tree with linear tree models and well-calibrated uncertainty estimates, implemented in the `lolopy` library. RF is an ensemble learning method that constructs multiple decision trees and combines their output for improved prediction accuracy and reduced overfitting. The M5-Tree with linear tree models enhances the standard RF by incorporating linear regression models in the tree leaves, providing better performance on certain types of data. \n",
    "\n",
    "EDIT: No longer uses 'lolopy' library\n",
    "\n",
    "All training and test sets used for the experiments will be stored in the `results` folder, allowing for easy access and reproducibility of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = os.path.join('data', 'numeric_data.csv')\n",
    "data = pd.read_csv(file_path)  \n",
    "        \n",
    "\n",
    "# DataFrames to store results\n",
    "train_results_df = pd.DataFrame()\n",
    "test_results_df = pd.DataFrame(columns=['Iteration', 'Idx_Sample', 'Input Features', 'True Values', 'Predicted Values'])\n",
    "\n",
    "n = 10\n",
    "N = 25\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "\n",
    "for i in range(10):\n",
    "    random.seed(i)\n",
    "    # np.random.seed(i)\n",
    "\n",
    "    #random_indices = np.random.choice(len(data), n+N, False)\n",
    "    random.shuffle(indices)\n",
    "    # Sample the data based on the provided indices\n",
    "    train_data = data.iloc[indices[:n]]\n",
    "    test_data = data.iloc[indices[n:n+N]]\n",
    "\n",
    "    target_column = 'fc_28dGroundTruth'\n",
    "    idx_column = 'Idx_Sample'\n",
    "    X_train = train_data.drop(columns=[target_column, idx_column], axis=1)\n",
    "\n",
    "    # Normalize input features\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train = X_scaler.fit_transform(X_train)\n",
    "\n",
    "    # Scale the target variable for training\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train = y_scaler.fit_transform(train_data[target_column].copy().to_numpy().reshape(-1, 1))\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Test data\n",
    "    X_test = test_data.drop(columns=[target_column, idx_column], axis=1)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "\n",
    "    # Predict on test data\n",
    "    predictions = rf.predict(X_test)\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    predictions = y_scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Store true and predicted values\n",
    "    true_values = test_data[target_column].copy().to_numpy().reshape(-1, 1)\n",
    "    idx_sample = test_data[idx_column].copy().to_numpy()\n",
    "\n",
    "    # Store train data\n",
    "    #train_results_df = train_results_df.append(train_data)\n",
    "    train_results_df = pd.concat([train_results_df, train_data], ignore_index=True)\n",
    "\n",
    "    # Store test data\n",
    "    iteration_df = pd.DataFrame({\n",
    "        'Iteration': i+1,\n",
    "        'Idx_Sample': idx_sample,\n",
    "        'Input Features': list(X_test),\n",
    "        'True Values': true_values.flatten(),\n",
    "        'Predicted Values': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    test_results_df = pd.concat([test_results_df, iteration_df], ignore_index=True)\n",
    "\n",
    "    # Calculate R2 score and mean absolute error\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    mae = mean_absolute_error(true_values, predictions)   \n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"Iteration: {i+1}\")\n",
    "    print(f\"R-squared: {r2:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "\n",
    "\n",
    "train_results_file = os.path.join('results', model_name, 'RF', 'train.csv')\n",
    "\n",
    "dir_name = os.path.dirname(train_results_file)\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "train_results_df.to_csv(train_results_file, index=False)\n",
    "\n",
    "test_results_file = os.path.join('results', model_name, 'RF', 'test.csv')\n",
    "test_results_df.to_csv(test_results_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
