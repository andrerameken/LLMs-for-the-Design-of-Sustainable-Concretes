{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19eec29d",
   "metadata": {},
   "source": [
    "## In-Context Learning (ICL) with contextual information\n",
    "\n",
    "In this section, we used the In-Context Learning (ICL) model using additional contextual information. By incorporating general knowledge about geopolymer concrete, we aim to enhance the model's predictive capabilities and achieve better performance than the traditional models. The fine-tuning prompt considers the following factors for geopolymer concrete with FA/GGBFS:\n",
    "\n",
    "1. Higher GGBFS content (e.g., 0.3/0.7) typically yields greater strength.\n",
    "2. Lower W/C ratios generally result in increased compressive strength.\n",
    "3. Enhanced powder content (FA + GGBFS) contributes to higher strength.\n",
    "4. Curing methods: Heat curing is often more effective for higher FA content (e.g., 0.5/0.5), while GGBFS-rich blends can achieve adequate strength with ambient curing.\n",
    "\n",
    "\n",
    "\n",
    "The In-Content Learning process involves providing the model with concrete formulations as prompts and their respective compressive strengths as completions for learning. Subsequently, the model will receive only prompts and will be required to generate completions itself. \n",
    "\n",
    "The prompt format is as follows:\n",
    "\n",
    "Please consider the following disclaimer: For geopolymer concrete with FA/GGBFS, consider the following: (1) Higher GGBFS content (e.g., 0.3/0.7) typically yields greater strength. (2) Lower W/C ratios generally result in increased compressive strength. (3) Enhanced powder content (FA + GGBFS) contributes to higher strength. (4) Curing methods: Heat curing is often more effective for higher FA content (e.g., 0.5/0.5), while GGBFS-rich blends can achieve adequate strength with ambient curing.; We will do an exercise where I will provide you with concrete formulations as prompts and their respective respective compressive strength as completions for you to learn from. Then you will only receive prompts and need to complete it yourself. Add the respective Idx to each answer. Let's go:\n",
    "\n",
    "{training_text}\n",
    "\n",
    "\n",
    "All training and test sets used for the experiments will be stored in the `results` folder, allowing for easy access and reproducibility of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308f48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: 55.12\n",
      "result: 55.12\n",
      "response: 45.88\n",
      "result: 45.88\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 47.97\n",
      "result: 47.97\n",
      "response: 60.71\n",
      "result: 60.71\n",
      "response: 60.71\n",
      "result: 60.71\n",
      "response: 57.14\n",
      "result: 57.14\n",
      "response: 49.12\n",
      "result: 49.12\n",
      "response: 50.12\n",
      "result: 50.12\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 52.08\n",
      "result: 52.08\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 68.57\n",
      "result: 68.57\n",
      "response: 47.62\n",
      "result: 47.62\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 47.56\n",
      "result: 47.56\n",
      "response: 54.12\n",
      "result: 54.12\n",
      "response: 57.14\n",
      "result: 57.14\n",
      "response: 49.12\n",
      "result: 49.12\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 60.71\n",
      "result: 60.71\n",
      "response: 45.88\n",
      "result: 45.88\n",
      "response: 54.12\n",
      "result: 54.12\n",
      "response: 41.86\n",
      "result: 41.86\n",
      "response: 49.17\n",
      "result: 49.17\n",
      "R-squared: 0.13\n",
      "MAE: 7.96\n",
      "MSE: 119.52\n",
      "response: 57.14\n",
      "result: 57.14\n",
      "response: 24.91\n",
      "result: 24.91\n",
      "response: 33.33\n",
      "result: 33.33\n",
      "response: 49.81\n",
      "result: 49.81\n",
      "response: 35.91\n",
      "result: 35.91\n",
      "response: 24.91\n",
      "result: 24.91\n",
      "response: 24.91\n",
      "result: 24.91\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 70.59\n",
      "result: 70.59\n",
      "response: 33.91\n",
      "result: 33.91\n",
      "response: 25.81\n",
      "result: 25.81\n",
      "response: 44.91\n",
      "result: 44.91\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 33.33\n",
      "result: 33.33\n",
      "response: 38.91\n",
      "result: 38.91\n",
      "response: 38.64\n",
      "result: 38.64\n",
      "response: 33.12\n",
      "result: 33.12\n",
      "response: 53.13\n",
      "result: 53.13\n",
      "response: 62.85\n",
      "result: 62.85\n",
      "response: 61.61\n",
      "result: 61.61\n",
      "response: 63.64\n",
      "result: 63.64\n",
      "response: 25.71\n",
      "result: 25.71\n",
      "response: 50.00\n",
      "result: 50.0\n",
      "R-squared: 0.47\n",
      "MAE: 5.56\n",
      "MSE: 54.10\n",
      "response: 60.0\n",
      "result: 60.0\n",
      "response: 68.89\n",
      "result: 68.89\n",
      "response: 52.93\n",
      "result: 52.93\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 54.55\n",
      "result: 54.55\n",
      "response: 35.4\n",
      "result: 35.4\n",
      "response: 41.4\n",
      "result: 41.4\n",
      "response: 35.7\n",
      "result: 35.7\n",
      "response: 52.8\n",
      "result: 52.8\n",
      "response: 47.2\n",
      "result: 47.2\n",
      "response: 45.97\n",
      "result: 45.97\n",
      "response: 35.64\n",
      "result: 35.64\n",
      "response: 60.0\n",
      "result: 60.0\n",
      "response: 46.08\n",
      "result: 46.08\n",
      "response: 33.75\n",
      "result: 33.75\n",
      "response: 54.0\n",
      "result: 54.0\n",
      "response: 38.72\n",
      "result: 38.72\n",
      "response: 51.48\n",
      "result: 51.48\n",
      "response: 60.0\n",
      "result: 60.0\n",
      "response: 35.7\n",
      "result: 35.7\n",
      "response: 60.0\n",
      "result: 60.0\n",
      "response: 35.7\n",
      "result: 35.7\n",
      "response: 68.75\n",
      "result: 68.75\n",
      "response: 49.59\n",
      "result: 49.59\n",
      "response: 33.33\n",
      "result: 33.33\n",
      "R-squared: 0.24\n",
      "MAE: 5.43\n",
      "MSE: 56.80\n",
      "response: 54.49\n",
      "result: 54.49\n",
      "response: 54.05\n",
      "result: 54.05\n",
      "response: 50.41\n",
      "result: 50.41\n",
      "response: 44.32\n",
      "result: 44.32\n",
      "response: 44.27\n",
      "result: 44.27\n",
      "response: 50.41\n",
      "result: 50.41\n",
      "response: 53.41\n",
      "result: 53.41\n",
      "response: 44.86\n",
      "result: 44.86\n",
      "response: 59.09\n",
      "result: 59.09\n",
      "response: 44.49\n",
      "result: 44.49\n",
      "response: 57.27\n",
      "result: 57.27\n",
      "response: 60.0\n",
      "result: 60.0\n",
      "response: 53.33\n",
      "result: 53.33\n",
      "response: 50.41\n",
      "result: 50.41\n",
      "response: 35.41\n",
      "result: 35.41\n",
      "response: 59.41\n",
      "result: 59.41\n",
      "response: 64.09\n",
      "result: 64.09\n",
      "response: 54.86\n",
      "result: 54.86\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 50.27\n",
      "result: 50.27\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 50.41\n",
      "result: 50.41\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 50.41\n",
      "result: 50.41\n",
      "response: 46.49\n",
      "result: 46.49\n",
      "R-squared: 0.40\n",
      "MAE: 5.91\n",
      "MSE: 55.50\n",
      "response: 47.71\n",
      "result: 47.71\n",
      "response: 38.97\n",
      "result: 38.97\n",
      "response: 66.08\n",
      "result: 66.08\n",
      "response: 38.97\n",
      "result: 38.97\n",
      "response: 45.97\n",
      "result: 45.97\n",
      "response: 52.13\n",
      "result: 52.13\n",
      "response: 31.76\n",
      "result: 31.76\n",
      "response: 50.4\n",
      "result: 50.4\n",
      "response: 50.44\n",
      "result: 50.44\n",
      "response: 31.85\n",
      "result: 31.85\n",
      "response: 31.08\n",
      "result: 31.08\n",
      "response: 60.08\n",
      "result: 60.08\n",
      "response: 50.97\n",
      "result: 50.97\n",
      "response: 50.08\n",
      "result: 50.08\n",
      "response: 33.33\n",
      "result: 33.33\n",
      "response: 44.85\n",
      "result: 44.85\n",
      "response: 50.97\n",
      "result: 50.97\n",
      "response: 44.85\n",
      "result: 44.85\n",
      "response: 50.4\n",
      "result: 50.4\n",
      "response: 50.97\n",
      "result: 50.97\n",
      "response: 31.68\n",
      "result: 31.68\n",
      "response: 33.97\n",
      "result: 33.97\n",
      "response: 68.32\n",
      "result: 68.32\n",
      "response: 66.5\n",
      "result: 66.5\n",
      "response: 60.72\n",
      "result: 60.72\n",
      "R-squared: 0.60\n",
      "MAE: 4.95\n",
      "MSE: 44.19\n",
      "response: 49.85\n",
      "result: 49.85\n",
      "response: 52.14\n",
      "result: 52.14\n",
      "response: 34.85\n",
      "result: 34.85\n",
      "response: 43.91\n",
      "result: 43.91\n",
      "response: 59.98\n",
      "result: 59.98\n",
      "response: 62.14\n",
      "result: 62.14\n",
      "response: 52.14\n",
      "result: 52.14\n",
      "response: 35.91\n",
      "result: 35.91\n",
      "response: 50.69\n",
      "result: 50.69\n",
      "response: 55.08\n",
      "result: 55.08\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 53.85\n",
      "result: 53.85\n",
      "response: 47.85\n",
      "result: 47.85\n",
      "response: 54.08\n",
      "result: 54.08\n",
      "response: 49.12\n",
      "result: 49.12\n",
      "response: 41.89\n",
      "result: 41.89\n",
      "response: 54.12\n",
      "result: 54.12\n",
      "response: 52.44\n",
      "result: 52.44\n",
      "response: 53.89\n",
      "result: 53.89\n",
      "response: 52.14\n",
      "result: 52.14\n",
      "response: 60.12\n",
      "result: 60.12\n",
      "response: 44.91\n",
      "result: 44.91\n",
      "response: 53.91\n",
      "result: 53.91\n",
      "response: 56.08\n",
      "result: 56.08\n",
      "response: 62.14\n",
      "result: 62.14\n",
      "R-squared: 0.34\n",
      "MAE: 6.37\n",
      "MSE: 59.27\n",
      "response: 44.12\n",
      "result: 44.12\n",
      "response: 28.57\n",
      "result: 28.57\n",
      "response: 68.97\n",
      "result: 68.97\n",
      "response: 35.2\n",
      "result: 35.2\n",
      "response: 30.12\n",
      "result: 30.12\n",
      "response: 68.97\n",
      "result: 68.97\n",
      "response: 41.86\n",
      "result: 41.86\n",
      "response: 30.17\n",
      "result: 30.17\n",
      "response: 50.12\n",
      "result: 50.12\n",
      "response: 31.25\n",
      "result: 31.25\n",
      "response: 70.2\n",
      "result: 70.2\n",
      "response: 49.14\n",
      "result: 49.14\n",
      "response: 22.86\n",
      "result: 22.86\n",
      "response: 68.97\n",
      "result: 68.97\n",
      "response: 35.12\n",
      "result: 35.12\n",
      "response: 44.12\n",
      "result: 44.12\n",
      "response: 68.97\n",
      "result: 68.97\n",
      "response: 60.08\n",
      "result: 60.08\n",
      "response: 68.97\n",
      "result: 68.97\n",
      "response: 61.86\n",
      "result: 61.86\n",
      "response: 31.86\n",
      "result: 31.86\n",
      "response: 41.23\n",
      "result: 41.23\n",
      "response: 60.17\n",
      "result: 60.17\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 50.91\n",
      "result: 50.91\n",
      "R-squared: 0.52\n",
      "MAE: 6.31\n",
      "MSE: 65.69\n",
      "response: 49.63\n",
      "result: 49.63\n",
      "response: 44.12\n",
      "result: 44.12\n",
      "response: 40.12\n",
      "result: 40.12\n",
      "response: 53.33\n",
      "result: 53.33\n",
      "response: 40.74\n",
      "result: 40.74\n",
      "response: 49.68\n",
      "result: 49.68\n",
      "response: 33.33\n",
      "result: 33.33\n",
      "response: 44.12\n",
      "result: 44.12\n",
      "response: 49.85\n",
      "result: 49.85\n",
      "response: 49.68\n",
      "result: 49.68\n",
      "response: 49.52\n",
      "result: 49.52\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 45.16\n",
      "result: 45.16\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 33.87\n",
      "result: 33.87\n",
      "response: 44.85\n",
      "result: 44.85\n",
      "response: 52.94\n",
      "result: 52.94\n",
      "response: 60.8\n",
      "result: 60.8\n",
      "response: 40.74\n",
      "result: 40.74\n",
      "response: 52.94\n",
      "result: 52.94\n",
      "response: 47.78\n",
      "result: 47.78\n",
      "response: 48.39\n",
      "result: 48.39\n",
      "response: 53.33\n",
      "result: 53.33\n",
      "R-squared: 0.29\n",
      "MAE: 6.68\n",
      "MSE: 69.65\n",
      "response: 44.97\n",
      "result: 44.97\n",
      "response: 60.08\n",
      "result: 60.08\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 56.67\n",
      "result: 56.67\n",
      "response: 55.08\n",
      "result: 55.08\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 36.72\n",
      "result: 36.72\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 56.44\n",
      "result: 56.44\n",
      "response: 56.44\n",
      "result: 56.44\n",
      "response: 55.08\n",
      "result: 55.08\n",
      "response: 41.86\n",
      "result: 41.86\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 56.12\n",
      "result: 56.12\n",
      "response: 55.86\n",
      "result: 55.86\n",
      "response: 36.97\n",
      "result: 36.97\n",
      "response: 35.85\n",
      "result: 35.85\n",
      "response: 55.12\n",
      "result: 55.12\n",
      "response: 44.81\n",
      "result: 44.81\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 44.85\n",
      "result: 44.85\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "R-squared: 0.12\n",
      "MAE: 8.26\n",
      "MSE: 101.96\n",
      "response: 38.89\n",
      "result: 38.89\n",
      "response: 55.12\n",
      "result: 55.12\n",
      "response: 49.08\n",
      "result: 49.08\n",
      "response: 38.97\n",
      "result: 38.97\n",
      "response: 36.89\n",
      "result: 36.89\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 55.81\n",
      "result: 55.81\n",
      "response: 50.2\n",
      "result: 50.2\n",
      "response: 54.23\n",
      "result: 54.23\n",
      "response: 44.81\n",
      "result: 44.81\n",
      "response: 45.35\n",
      "result: 45.35\n",
      "response: 45.08\n",
      "result: 45.08\n",
      "response: 60.91\n",
      "result: 60.91\n",
      "response: 38.68\n",
      "result: 38.68\n",
      "response: 39.68\n",
      "result: 39.68\n",
      "response: 30.91\n",
      "result: 30.91\n",
      "response: 47.08\n",
      "result: 47.08\n",
      "response: 44.32\n",
      "result: 44.32\n",
      "response: 57.37\n",
      "result: 57.37\n",
      "response: 38.97\n",
      "result: 38.97\n",
      "response: 40.12\n",
      "result: 40.12\n",
      "response: 56.25\n",
      "result: 56.25\n",
      "response: 55.56\n",
      "result: 55.56\n",
      "response: 60.86\n",
      "result: 60.86\n",
      "response: 47.89\n",
      "result: 47.89\n",
      "R-squared: 0.63\n",
      "MAE: 4.67\n",
      "MSE: 48.07\n",
      "Results for 250 iterations are saved to a single CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Predict Alkali activated concrete properties with in-context learning using openAI's text-davinci-003 model\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai import RateLimitError\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from utils import *\n",
    "\n",
    "# Load OpenAI client\n",
    "client = OpenAI()\n",
    "model_name = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "# Read data from file\n",
    "data_path = os.path.join('data', 'transformed_data.txt')\n",
    "with open(data_path, 'r') as f:\n",
    "    data = f.readlines()    \n",
    "\n",
    "context_prompt = \"Please consider the following disclaimer: For geopolymer concrete with FA/GGBFS, consider the following: (1) Higher GGBFS content (e.g., 0.3/0.7) typically yields greater strength. (2) Lower W/C ratios generally result in increased compressive strength. (3) Enhanced powder content (FA + GGBFS) contributes to higher strength. (4) Curing methods: Heat curing is often more effective for higher FA content (e.g., 0.5/0.5), while GGBFS-rich blends can achieve adequate strength with ambient curing.; We will do an exercise where I will provide you with concrete formulations as prompts and their respective respective compressive strength as completions for you to learn from. Then you will only receive prompts and need to complete it yourself with a single number. Lets go:\"\n",
    "# Randomly sample n lines for training and N lines for testing\n",
    "n = 10\n",
    "N = 25\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "result_list = []\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "# Repeat the process 10 times\n",
    "for i in range(10):\n",
    "    #np.random.seed(i)\n",
    "    random.seed(i)\n",
    "    test_prompts, true_values, predictions = gather_LLM_results(data,\n",
    "                                                                n,\n",
    "                                                                N,\n",
    "                                                                client,\n",
    "                                                                model_name,\n",
    "                                                                indices,\n",
    "                                                                context_prompt=context_prompt)\n",
    "\n",
    "    append_to_result_list(test_prompts, true_values, predictions, result_list)\n",
    "\n",
    "\n",
    "save_results_to_csv(model_name, \"ICL_finetuned\", result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ceac0",
   "metadata": {},
   "source": [
    "### Vanilla In-Context Learning (ICL)\n",
    "\n",
    "In this section, we will benchmark the performance of the In-Context Learning (ICL) model using the text-davinci-003 model from OpenAI. ICL leverages large language models to incorporate context and general knowledge, providing flexibility in handling non-numeric inputs and overcoming the limitations of traditional vector space formulations. The experiments will be conducted using the OpenAI API without any additional fine-tuning or contextual information.\n",
    "\n",
    "All training and test sets used for the experiments will be stored in the `results` folder, allowing for easy access and reproducibility of the study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803349e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: 54.12\n",
      "result: 54.12\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 41.12\n",
      "result: 41.12\n",
      "response: 47.62\n",
      "result: 47.62\n",
      "response: 59.12\n",
      "result: 59.12\n",
      "response: 59.09\n",
      "result: 59.09\n",
      "response: 55.26\n",
      "result: 55.26\n",
      "response: 47.12\n",
      "result: 47.12\n",
      "response: 41.92\n",
      "result: 41.92\n",
      "response: 47.62\n",
      "result: 47.62\n",
      "response: 52.94\n",
      "result: 52.94\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 68.57\n",
      "result: 68.57\n",
      "response: 49.41\n",
      "result: 49.41\n",
      "response: 47.62\n",
      "result: 47.62\n",
      "response: 49.12\n",
      "result: 49.12\n",
      "response: 54.12\n",
      "result: 54.12\n",
      "response: 54.55\n",
      "result: 54.55\n",
      "response: 59.09\n",
      "result: 59.09\n",
      "response: 51.11\n",
      "result: 51.11\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 45.88\n",
      "result: 45.88\n",
      "response: 54.12\n",
      "result: 54.12\n",
      "response: 41.12\n",
      "result: 41.12\n",
      "response: 61.11\n",
      "result: 61.11\n",
      "R-squared: -0.57\n",
      "MAE: 10.64\n",
      "MSE: 215.33\n",
      "response: 57.14\n",
      "result: 57.14\n",
      "response: 35.29\n",
      "result: 35.29\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 49.82\n",
      "result: 49.82\n",
      "response: 35.12\n",
      "result: 35.12\n",
      "response: 35.12\n",
      "result: 35.12\n",
      "response: 35.12\n",
      "result: 35.12\n",
      "response: 49.81\n",
      "result: 49.81\n",
      "response: 49.88\n",
      "result: 49.88\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 58.91\n",
      "result: 58.91\n",
      "response: 35.12\n",
      "result: 35.12\n",
      "response: 38.91\n",
      "result: 38.91\n",
      "response: 41.89\n",
      "result: 41.89\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 38.91\n",
      "result: 38.91\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 33.12\n",
      "result: 33.12\n",
      "response: 26.81\n",
      "result: 26.81\n",
      "response: 60.89\n",
      "result: 60.89\n",
      "response: 61.61\n",
      "result: 61.61\n",
      "response: 63.64\n",
      "result: 63.64\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 50.12\n",
      "result: 50.12\n",
      "R-squared: 0.32\n",
      "MAE: 5.92\n",
      "MSE: 69.08\n",
      "response: 61.11\n",
      "result: 61.11\n",
      "response: 62.22\n",
      "result: 62.22\n",
      "response: 52.93\n",
      "result: 52.93\n",
      "response: 45.12\n",
      "result: 45.12\n",
      "response: 61.11\n",
      "result: 61.11\n",
      "response: 41.05\n",
      "result: 41.05\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 47.37\n",
      "result: 47.37\n",
      "response: 45.12\n",
      "result: 45.12\n",
      "response: 41.38\n",
      "result: 41.38\n",
      "response: 61.36\n",
      "result: 61.36\n",
      "response: 45.71\n",
      "result: 45.71\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 61.88\n",
      "result: 61.88\n",
      "response: 36.72\n",
      "result: 36.72\n",
      "response: 50.97\n",
      "result: 50.97\n",
      "response: 61.33\n",
      "result: 61.33\n",
      "response: 41.02\n",
      "result: 41.02\n",
      "response: 56.67\n",
      "result: 56.67\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 67.5\n",
      "result: 67.5\n",
      "response: 48.97\n",
      "result: 48.97\n",
      "response: 35.56\n",
      "result: 35.56\n",
      "R-squared: 0.08\n",
      "MAE: 6.39\n",
      "MSE: 69.04\n",
      "response: 55.14\n",
      "result: 55.14\n",
      "response: 54.95\n",
      "result: 54.95\n",
      "response: 49.95\n",
      "result: 49.95\n",
      "response: 45.45\n",
      "result: 45.45\n",
      "response: 49.41\n",
      "result: 49.41\n",
      "response: 54.55\n",
      "result: 54.55\n",
      "response: 54.05\n",
      "result: 54.05\n",
      "response: 45.14\n",
      "result: 45.14\n",
      "response: 59.09\n",
      "result: 59.09\n",
      "response: 44.49\n",
      "result: 44.49\n",
      "response: 54.55\n",
      "result: 54.55\n",
      "response: 49.09\n",
      "result: 49.09\n",
      "response: 55.41\n",
      "result: 55.41\n",
      "response: 49.72\n",
      "result: 49.72\n",
      "response: 38.72\n",
      "result: 38.72\n",
      "response: 59.41\n",
      "result: 59.41\n",
      "response: 60.0\n",
      "result: 60.0\n",
      "response: 43.58\n",
      "result: 43.58\n",
      "response: 49.41\n",
      "result: 49.41\n",
      "response: 49.52\n",
      "result: 49.52\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 53.33\n",
      "result: 53.33\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 50.41\n",
      "result: 50.41\n",
      "response: 49.41\n",
      "result: 49.41\n",
      "R-squared: 0.09\n",
      "MAE: 7.11\n",
      "MSE: 83.62\n",
      "response: 49.82\n",
      "result: 49.82\n",
      "response: 38.29\n",
      "result: 38.29\n",
      "response: 56.92\n",
      "result: 56.92\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 50.23\n",
      "result: 50.23\n",
      "response: 52.94\n",
      "result: 52.94\n",
      "response: 31.72\n",
      "result: 31.72\n",
      "response: 50.72\n",
      "result: 50.72\n",
      "response: 50.36\n",
      "result: 50.36\n",
      "response: 29.72\n",
      "result: 29.72\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 57.29\n",
      "result: 57.29\n",
      "response: 50.36\n",
      "result: 50.36\n",
      "response: 51.02\n",
      "result: 51.02\n",
      "response: 38.24\n",
      "result: 38.24\n",
      "response: 45.92\n",
      "result: 45.92\n",
      "response: 50.23\n",
      "result: 50.23\n",
      "response: 50.02\n",
      "result: 50.02\n",
      "response: 38.89\n",
      "result: 38.89\n",
      "response: 50.23\n",
      "result: 50.23\n",
      "response: 31.25\n",
      "result: 31.25\n",
      "response: 33.72\n",
      "result: 33.72\n",
      "response: 35.71\n",
      "result: 35.71\n",
      "response: 60.08\n",
      "result: 60.08\n",
      "response: 56.72\n",
      "result: 56.72\n",
      "R-squared: 0.57\n",
      "MAE: 5.64\n",
      "MSE: 48.13\n",
      "response: 47.89\n",
      "result: 47.89\n",
      "response: 49.81\n",
      "result: 49.81\n",
      "response: 34.85\n",
      "result: 34.85\n",
      "response: 42.91\n",
      "result: 42.91\n",
      "response: 53.85\n",
      "result: 53.85\n",
      "response: 62.31\n",
      "result: 62.31\n",
      "response: 49.81\n",
      "result: 49.81\n",
      "response: 34.91\n",
      "result: 34.91\n",
      "response: 50.69\n",
      "result: 50.69\n",
      "response: 53.12\n",
      "result: 53.12\n",
      "response: 41.12\n",
      "result: 41.12\n",
      "response: 53.12\n",
      "result: 53.12\n",
      "response: 47.85\n",
      "result: 47.85\n",
      "response: 52.85\n",
      "result: 52.85\n",
      "response: 49.91\n",
      "result: 49.91\n",
      "response: 41.89\n",
      "result: 41.89\n",
      "response: 53.12\n",
      "result: 53.12\n",
      "response: 49.12\n",
      "result: 49.12\n",
      "response: 53.12\n",
      "result: 53.12\n",
      "response: 47.12\n",
      "result: 47.12\n",
      "response: 49.81\n",
      "result: 49.81\n",
      "response: 44.12\n",
      "result: 44.12\n",
      "response: 52.14\n",
      "result: 52.14\n",
      "response: 55.12\n",
      "result: 55.12\n",
      "response: 62.14\n",
      "result: 62.14\n",
      "R-squared: 0.33\n",
      "MAE: 6.43\n",
      "MSE: 59.82\n",
      "response: 41.86\n",
      "result: 41.86\n",
      "response: 28.57\n",
      "result: 28.57\n",
      "response: 68.75\n",
      "result: 68.75\n",
      "response: 60.71\n",
      "result: 60.71\n",
      "response: 41.23\n",
      "result: 41.23\n",
      "response: 68.57\n",
      "result: 68.57\n",
      "response: 68.12\n",
      "result: 68.12\n",
      "response: 30.12\n",
      "result: 30.12\n",
      "response: 41.23\n",
      "result: 41.23\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 68.57\n",
      "result: 68.57\n",
      "response: 45.12\n",
      "result: 45.12\n",
      "response: 29.63\n",
      "result: 29.63\n",
      "response: 68.18\n",
      "result: 68.18\n",
      "response: 41.72\n",
      "result: 41.72\n",
      "response: 45.72\n",
      "result: 45.72\n",
      "response: 68.72\n",
      "result: 68.72\n",
      "response: 60.71\n",
      "result: 60.71\n",
      "response: 68.12\n",
      "result: 68.12\n",
      "response: 61.11\n",
      "result: 61.11\n",
      "response: 41.12\n",
      "result: 41.12\n",
      "response: 41.23\n",
      "result: 41.23\n",
      "response: 68.29\n",
      "result: 68.29\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 41.38\n",
      "result: 41.38\n",
      "R-squared: 0.22\n",
      "MAE: 8.65\n",
      "MSE: 107.68\n",
      "response: 59.12\n",
      "result: 59.12\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 43.75\n",
      "result: 43.75\n",
      "response: 53.72\n",
      "result: 53.72\n",
      "response: 40.12\n",
      "result: 40.12\n",
      "response: 50.61\n",
      "result: 50.61\n",
      "response: 39.13\n",
      "result: 39.13\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 59.12\n",
      "result: 59.12\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 47.62\n",
      "result: 47.62\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 44.64\n",
      "result: 44.64\n",
      "response: 59.09\n",
      "result: 59.09\n",
      "response: 39.13\n",
      "result: 39.13\n",
      "response: 44.92\n",
      "result: 44.92\n",
      "response: 49.12\n",
      "result: 49.12\n",
      "response: 60.87\n",
      "result: 60.87\n",
      "response: 40.12\n",
      "result: 40.12\n",
      "response: 52.94\n",
      "result: 52.94\n",
      "response: 47.85\n",
      "result: 47.85\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 53.33\n",
      "result: 53.33\n",
      "R-squared: -0.32\n",
      "MAE: 8.50\n",
      "MSE: 129.51\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 55.71\n",
      "result: 55.71\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 55.33\n",
      "result: 55.33\n",
      "response: 52.87\n",
      "result: 52.87\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 41.11\n",
      "result: 41.11\n",
      "response: 55.08\n",
      "result: 55.08\n",
      "response: 55.56\n",
      "result: 55.56\n",
      "response: 50.12\n",
      "result: 50.12\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 55.44\n",
      "result: 55.44\n",
      "response: 38.97\n",
      "result: 38.97\n",
      "response: 36.45\n",
      "result: 36.45\n",
      "response: 55.12\n",
      "result: 55.12\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 41.86\n",
      "result: 41.86\n",
      "response: 44.44\n",
      "result: 44.44\n",
      "response: 41.67\n",
      "result: 41.67\n",
      "response: 46.67\n",
      "result: 46.67\n",
      "response: 41.85\n",
      "result: 41.85\n",
      "R-squared: -0.02\n",
      "MAE: 8.84\n",
      "MSE: 118.57\n",
      "response: 49.52\n",
      "result: 49.52\n",
      "response: 49.23\n",
      "result: 49.23\n",
      "response: 45.12\n",
      "result: 45.12\n",
      "response: 49.09\n",
      "result: 49.09\n",
      "response: 36.11\n",
      "result: 36.11\n",
      "response: 49.23\n",
      "result: 49.23\n",
      "response: 55.12\n",
      "result: 55.12\n",
      "response: 50.00\n",
      "result: 50.0\n",
      "response: 55.38\n",
      "result: 55.38\n",
      "response: 49.23\n",
      "result: 49.23\n",
      "response: 45.45\n",
      "result: 45.45\n",
      "response: 45.23\n",
      "result: 45.23\n",
      "response: 59.12\n",
      "result: 59.12\n",
      "response: 38.68\n",
      "result: 38.68\n",
      "response: 49.74\n",
      "result: 49.74\n",
      "response: 30.00\n",
      "result: 30.0\n",
      "response: 49.12\n",
      "result: 49.12\n",
      "response: 52.73\n",
      "result: 52.73\n",
      "response: 55.12\n",
      "result: 55.12\n",
      "response: 45.35\n",
      "result: 45.35\n",
      "response: 40.12\n",
      "result: 40.12\n",
      "response: 50.00\n",
      "result: 50.0\n",
      "response: 50.00\n",
      "result: 50.0\n",
      "response: 54.29\n",
      "result: 54.29\n",
      "response: 38.29\n",
      "result: 38.29\n",
      "R-squared: 0.23\n",
      "MAE: 7.32\n",
      "MSE: 99.57\n",
      "Results for 250 iterations are saved to a single CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Predict Alkali activated concrete properties with in-context learning using openAI's text-davinci-003 model\n",
    "\n",
    "from openai import OpenAI\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "from utils import *\n",
    "\n",
    "# Load API key\n",
    "client = OpenAI()\n",
    "model_name = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "data_path = os.path.join('data', 'transformed_data.txt')\n",
    "\n",
    "context_prompt = \"We will do an exercise where I will provide you with concrete formulations as prompts and their respective respective compressive strength as completions for you to learn from. Then you will only receive prompts and need to complete it yourself with a single number. Lets go:\"\n",
    "\n",
    "# Read data from file\n",
    "with open(data_path, 'r') as f:\n",
    "    data = f.readlines()\n",
    "# Randomly sample n lines for training and N lines for testing\n",
    "n = 10\n",
    "N = 25\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "result_list = []\n",
    "indices = list(range(len(data)))\n",
    "# Repeat the process 10 times\n",
    "for i in range(10):\n",
    "    #np.random.seed(i)\n",
    "    random.seed(i)\n",
    "    test_prompts, true_values, predictions = gather_LLM_results(data,\n",
    "                                                                n,\n",
    "                                                                N,\n",
    "                                                                client,\n",
    "                                                                model_name,\n",
    "                                                                indices,\n",
    "                                                                context_prompt=context_prompt)\n",
    "\n",
    "    append_to_result_list(test_prompts, true_values, predictions, result_list)\n",
    "\n",
    "\n",
    "save_results_to_csv(model_name, \"ICL\", result_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d351db",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression (GPR)\n",
    "\n",
    "In this section, we will benchmark the performance of the Gaussian Process Regression (GPR) model using the `scikit-learn` library. GPR is a non-parametric, Bayesian approach to regression that provides uncertainty estimates of the predictions. It is based on the assumption that any finite set of data points can be modeled by a multivariate Gaussian distribution.\n",
    "\n",
    "All training and test sets used for the experiments will be stored in the `results` folder, allowing for easy access and reproducibility of the study.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2a0837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "R-squared: 0.78\n",
      "MAE: 4.35\n",
      "MSE: 29.76\n",
      "Iteration: 2\n",
      "R-squared: 0.01\n",
      "MAE: 8.30\n",
      "MSE: 100.57\n",
      "Iteration: 3\n",
      "R-squared: 0.62\n",
      "MAE: 4.34\n",
      "MSE: 28.64\n",
      "Iteration: 4\n",
      "R-squared: 0.35\n",
      "MAE: 6.41\n",
      "MSE: 59.51\n",
      "Iteration: 5\n",
      "R-squared: 0.28\n",
      "MAE: 7.41\n",
      "MSE: 80.64\n",
      "Iteration: 6\n",
      "R-squared: 0.64\n",
      "MAE: 4.89\n",
      "MSE: 31.87\n",
      "Iteration: 7\n",
      "R-squared: 0.78\n",
      "MAE: 4.45\n",
      "MSE: 30.08\n",
      "Iteration: 8\n",
      "R-squared: 0.69\n",
      "MAE: 4.66\n",
      "MSE: 30.84\n",
      "Iteration: 9\n",
      "R-squared: 0.48\n",
      "MAE: 6.17\n",
      "MSE: 59.95\n",
      "Iteration: 10\n",
      "R-squared: 0.65\n",
      "MAE: 5.70\n",
      "MSE: 45.49\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "#data = pd.read_csv(r'data/numeric_data.csv')\n",
    "file_path = os.path.join('data', 'numeric_data.csv')\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(length_scale=10, nu=1.5)\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "# DataFrames to store results\n",
    "train_results_df = pd.DataFrame()\n",
    "test_results_df = pd.DataFrame(columns=['Iteration', 'Idx_Sample', 'Input Features', 'True Values', 'Predicted Values'])\n",
    "\n",
    "n = 10\n",
    "N = 25\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "\n",
    "for i in range(10):\n",
    "    #np.random.seed(i)\n",
    "    random.seed(i)\n",
    "    #random_indices = np.random.choice(len(data), n+N, False)\n",
    "    random.shuffle(indices)\n",
    "    # Sample the data based on the provided indices\n",
    "    train_data = data.iloc[indices[:n]]\n",
    "    test_data = data.iloc[indices[n:n+N]]\n",
    "\n",
    "    target_column = 'fc_28dGroundTruth'\n",
    "    idx_column = 'Idx_Sample'\n",
    "    X_train = train_data.drop(columns=[target_column, idx_column], axis=1)\n",
    "\n",
    "    # Normalize input features\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train = X_scaler.fit_transform(X_train)\n",
    "\n",
    "    # Scale the target variable for training\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train = y_scaler.fit_transform(train_data[target_column].copy().to_numpy().reshape(-1, 1))\n",
    "\n",
    "    gpr.fit(X_train, y_train)\n",
    "\n",
    "    # Test data\n",
    "    X_test = test_data.drop(columns=[target_column, idx_column], axis=1)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "\n",
    "    # Predict on test data\n",
    "    predictions = gpr.predict(X_test)\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    predictions = y_scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Store true and predicted values\n",
    "    true_values = test_data[target_column].copy().to_numpy().reshape(-1, 1)\n",
    "    idx_sample = test_data[idx_column].copy().to_numpy()\n",
    "\n",
    "    # Store train data\n",
    "    #train_results_df = train_results_df.append(train_data)\n",
    "    train_results_df = pd.concat([train_results_df, train_data], ignore_index=True)\n",
    "\n",
    "    # Store test data\n",
    "    iteration_df = pd.DataFrame({\n",
    "        'Iteration': i+1,\n",
    "        'Idx_Sample': idx_sample,\n",
    "        'Input Features': list(X_test),\n",
    "        'True Values': true_values.flatten(),\n",
    "        'Predicted Values': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    test_results_df = pd.concat([test_results_df, iteration_df], ignore_index=True)\n",
    "\n",
    "    # Calculate R2 score and mean absolute error\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    mae = mean_absolute_error(true_values, predictions)   \n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"Iteration: {i+1}\")\n",
    "    print(f\"R-squared: {r2:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "train_results_file = os.path.join('results', model_name, 'GPR', 'train.csv')\n",
    "\n",
    "# Make needed directories\n",
    "dir_name = os.path.dirname(train_results_file)\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "train_results_df.to_csv(train_results_file, index=False)\n",
    "\n",
    "test_results_file = os.path.join('results', model_name, 'GPR', 'test.csv')\n",
    "test_results_df.to_csv(test_results_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8bbdc",
   "metadata": {},
   "source": [
    "### Random Forest (M5-Tree with Linear Tree Models)\n",
    "\n",
    "In this section, we will benchmark the performance of the Random Forest (RF) model using an M5-Tree with linear tree models and well-calibrated uncertainty estimates, implemented in the `lolopy` library. RF is an ensemble learning method that constructs multiple decision trees and combines their output for improved prediction accuracy and reduced overfitting. The M5-Tree with linear tree models enhances the standard RF by incorporating linear regression models in the tree leaves, providing better performance on certain types of data. \n",
    "\n",
    "EDIT: No longer uses 'lolopy' library\n",
    "\n",
    "All training and test sets used for the experiments will be stored in the `results` folder, allowing for easy access and reproducibility of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5405387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "R-squared: 0.67\n",
      "MAE: 5.53\n",
      "MSE: 45.62\n",
      "Iteration: 2\n",
      "R-squared: 0.70\n",
      "MAE: 4.57\n",
      "MSE: 30.68\n",
      "Iteration: 3\n",
      "R-squared: 0.44\n",
      "MAE: 4.91\n",
      "MSE: 41.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4\n",
      "R-squared: 0.43\n",
      "MAE: 6.06\n",
      "MSE: 52.67\n",
      "Iteration: 5\n",
      "R-squared: 0.65\n",
      "MAE: 4.88\n",
      "MSE: 39.37\n",
      "Iteration: 6\n",
      "R-squared: 0.64\n",
      "MAE: 4.45\n",
      "MSE: 32.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7\n",
      "R-squared: 0.71\n",
      "MAE: 5.10\n",
      "MSE: 39.98\n",
      "Iteration: 8\n",
      "R-squared: 0.59\n",
      "MAE: 5.05\n",
      "MSE: 40.36\n",
      "Iteration: 9\n",
      "R-squared: 0.48\n",
      "MAE: 6.15\n",
      "MSE: 59.76\n",
      "Iteration: 10\n",
      "R-squared: 0.51\n",
      "MAE: 6.42\n",
      "MSE: 64.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_6464\\2298538285.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(X_train, y_train)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = os.path.join('data', 'numeric_data.csv')\n",
    "data = pd.read_csv(file_path)  \n",
    "        \n",
    "\n",
    "# DataFrames to store results\n",
    "train_results_df = pd.DataFrame()\n",
    "test_results_df = pd.DataFrame(columns=['Iteration', 'Idx_Sample', 'Input Features', 'True Values', 'Predicted Values'])\n",
    "\n",
    "n = 10\n",
    "N = 25\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "\n",
    "for i in range(10):\n",
    "    #np.random.seed(i)\n",
    "    #random_indices = np.random.choice(range(0, len(data)), n+N, False)\n",
    "\n",
    "    random.seed(i)\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Sample the data based on the provided indices\n",
    "    train_data = data.iloc[indices[:n]]\n",
    "    test_data = data.iloc[indices[n:n+N]]\n",
    "\n",
    "    target_column = 'fc_28dGroundTruth'\n",
    "    idx_column = 'Idx_Sample'\n",
    "    X_train = train_data.drop(columns=[target_column, idx_column], axis=1)\n",
    "\n",
    "    # Normalize input features\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train = X_scaler.fit_transform(X_train)\n",
    "\n",
    "    # Scale the target variable for training\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train = y_scaler.fit_transform(train_data[target_column].copy().to_numpy().reshape(-1, 1))\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Test data\n",
    "    X_test = test_data.drop(columns=[target_column, idx_column], axis=1)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "\n",
    "    # Predict on test data\n",
    "    predictions = rf.predict(X_test)\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    predictions = y_scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Store true and predicted values\n",
    "    true_values = test_data[target_column].copy().to_numpy().reshape(-1, 1)\n",
    "    idx_sample = test_data[idx_column].copy().to_numpy()\n",
    "\n",
    "    # Store train data\n",
    "    #train_results_df = train_results_df.append(train_data)\n",
    "    train_results_df = pd.concat([train_results_df, train_data], ignore_index=True)\n",
    "\n",
    "    # Store test data\n",
    "    iteration_df = pd.DataFrame({\n",
    "        'Iteration': i+1,\n",
    "        'Idx_Sample': idx_sample,\n",
    "        'Input Features': list(X_test),\n",
    "        'True Values': true_values.flatten(),\n",
    "        'Predicted Values': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    test_results_df = pd.concat([test_results_df, iteration_df], ignore_index=True)\n",
    "\n",
    "    # Calculate R2 score and mean absolute error\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    mae = mean_absolute_error(true_values, predictions)   \n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"Iteration: {i+1}\")\n",
    "    print(f\"R-squared: {r2:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "\n",
    "\n",
    "train_results_file = os.path.join('results', model_name, 'RF', 'train.csv')\n",
    "\n",
    "dir_name = os.path.dirname(train_results_file)\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "train_results_df.to_csv(train_results_file, index=False)\n",
    "\n",
    "test_results_file = os.path.join('results', model_name, 'RF', 'test.csv')\n",
    "test_results_df.to_csv(test_results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365e929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text2Concrete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
